package org.example;

import java.io.*;
import java.util.Arrays;
import java.util.Iterator;

import com.amazonaws.regions.Regions;
import com.amazonaws.services.s3.AmazonS3;
import com.amazonaws.services.s3.AmazonS3ClientBuilder;
import com.amazonaws.services.s3.model.S3ObjectSummary;
import org.apache.commons.io.FileUtils;
import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.fs.FileStatus;
import org.apache.hadoop.fs.FileSystem;
import org.apache.hadoop.fs.Path;
import org.apache.hadoop.io.BytesWritable;
import org.apache.hadoop.io.IntWritable;
import org.apache.hadoop.io.SequenceFile;
import org.apache.hadoop.io.SequenceFile.CompressionType;
import org.apache.hadoop.io.SequenceFile.Writer;
import org.apache.log4j.Logger;
import org.apache.thrift.TException;
import org.apache.thrift.TSerializer;
import org.apache.thrift.protocol.TBinaryProtocol;

import edu.uchicago.mpcs53013.nvega.baseball.BaseballPlay;
import org.apache.thrift.transport.TTransportException;


public class SerializeBaseballPlay extends BaseSerializer<BaseballPlay> {

    public static void main(String[] args) throws Exception {
        if (args.length == 0 || args.length > 2) {
            System.err.println("Usage:  yarn jar jarname inputPath [hdfsOutputDir]");
            System.exit(-1);
        }

        new SerializeBaseballPlay().run(
                args[0],
                (args.length > 1) ? args[1] : "/nvega_data/thrift/plays"
        );
    }

    @Override
    protected BaseProcessor<BaseballPlay> createProcessor(Writer writer, TSerializer serializer) {
        return new BaseballPlayProcessor() {
            @Override
            protected void processRecord(BaseballPlay play, String sourceName) throws IOException {
                try {
                    writer.append(
                            new IntWritable(1),   // same constant key as your original
                            new BytesWritable(serializer.serialize(play))
                    );
                } catch (Exception e) {
                    throw new IOException(e);
                }
            }
        };
    }
}

// The classes below make it possible to iterate InputStreams for all the tar files in a folder.
// There is one class for each kind of folder that can hold the TarFiles (local filesystem, HDFS, and S3).

// All of these are iterable, so we can use a for loop as above to iterate through the tar files.
// This requires a lot of boilerplate implementing the Iterable and Iterator interfaces (e.g., hasNext()),
// but fortunately, that is basically passing through to the appropriate iterator for the folder type.

class InputStreamsForLocalDirectory implements Iterable<InputStream> {
	InputStreamsForLocalDirectory(String pathName) {
		this.pathName = pathName;
	}
	String pathName;

	@Override
	public Iterator<InputStream> iterator() {
		return new LocalInputStreamsIterator();
	}

	private class LocalInputStreamsIterator implements Iterator<InputStream> {
		Iterator<File> fileIterator
				= FileUtils.iterateFiles(new File(pathName), null, false);

		@Override
		public boolean hasNext() {
			return fileIterator.hasNext();
		}

		@Override
		public InputStream next() {
			try {
				return new FileInputStream(fileIterator.next());
			} catch (FileNotFoundException e) {
				e.printStackTrace();
			}
			return null;
		}
	}
}
/** Provides InputStreams for all the tar files in an HDFS directory
 *
 */
class InputStreamsForHdfsDirectory implements Iterable<InputStream> {
	/**
	 *
	 * @param fs The HDFS Filesystem containing the directory
	 * @param pathName The pathName of the directory
	 */
	InputStreamsForHdfsDirectory(FileSystem fs, String pathName) {
		this.fs = fs;
		String hdfsPrefix = "hdfs://";
		String withoutHdfsPrefix
				= pathName.startsWith(hdfsPrefix) ? pathName.substring(hdfsPrefix.length()) : pathName;

		this.path = new Path(withoutHdfsPrefix);
	}
	FileSystem fs;
	Path path;

	@Override
	public Iterator<InputStream> iterator() {
		return new HdfsInputStreamsIterator();
	}

	private class HdfsInputStreamsIterator implements Iterator<InputStream> {
		HdfsInputStreamsIterator() {
			try {
				fileStatusIterator = Arrays.asList(fs.listStatus(path)).iterator();
			} catch (IOException e) {
				e.printStackTrace();
			}
		}
		Iterator<FileStatus> fileStatusIterator;

		@Override
		public boolean hasNext() {
			return fileStatusIterator.hasNext();
		}

		@Override
		public InputStream next() {
			try {
				return fs.open(fileStatusIterator.next().getPath());
			} catch (IOException e) {
				e.printStackTrace();
			}
			return null;
		}

	}
}

class InputStreamsForS3Folder implements Iterable<InputStream> {
	static Logger log = Logger.getLogger("InputStreamsForS3Folder");
	InputStreamsForS3Folder(String folderName) {
		String s3Prefix = "s3://";
		String withoutS3Prefix
				= folderName.startsWith(s3Prefix) ? folderName.substring(s3Prefix.length()) : folderName;
		// Up to the first / is the bucket name
		bucketName = withoutS3Prefix.substring(0, withoutS3Prefix.indexOf('/'));
		// After the bucket name is the object prefix
		objectPrefix = withoutS3Prefix.substring(bucketName.length()+1);
	}
	String bucketName;
	String objectPrefix;
	AmazonS3 s3 = AmazonS3ClientBuilder.standard().withRegion(Regions.US_EAST_2).build();
	@Override
	public Iterator<InputStream> iterator() {
		return new S3InputStreamsIterator();
	}

	private class S3InputStreamsIterator implements Iterator<InputStream> {

		@Override
		public boolean hasNext() {
			return objectSummaryIterator.hasNext();
		}

		@Override
		public InputStream next() {
			return s3.getObject(bucketName, objectSummaryIterator.next().getKey()).getObjectContent();
		}

		Iterator<S3ObjectSummary> objectSummaryIterator
				= s3.listObjects(bucketName, objectPrefix).getObjectSummaries().iterator();
	}
}
